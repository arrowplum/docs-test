---
title: Recreating the Blog Post "Comparing NoSQL Databases&#58; Aerospike and Cassandra - Benchmarking for Real" with Manual Installation - Cassandra
description: Configuring Cassandra for a YCSB Session Store benchmark.
styles:
  - /assets/styles/ui/steps.css
---

You should already have completed all the benchmark tests with YCSB using Aerospike, and now be prepared to reset the database servers for testing Cassandra. These instructions assume that you have already gone through the preparation of the database servers for [Aerospike on AWS](/docs/benchmarks/cassandra/simple_ycsb/aerospike_aws.html) or a [manual Aerospike installation](/docs/benchmarks/cassandra/simple_ycsb/aerospike_install.html).


{{#steps}}

{{!---
  ############################################################################
  #
  # STEP 1 - Setting Up Cassandra's Prerequisites
  #
  ############################################################################
---}}
{{#steps-step 1 "Setting Up Cassandra's Prerequisites" markdown=true}}

## Stop iptables
By default, CentOS 6 has iptables turned on. This will stop incoming connections. Turn off iptables (if you have already installed Aerospike, you may already have completed this step):

```bash
dbhost$ sudo service iptables stop
dbhost$ sudo chkconfig iptables off
```
Alternatively, you may decide to leave iptables on. In this case, make sure you open the necessary ports for Cassandra: 7000, 7199, and 9042. Security on ports is one of the most common issues encountered when forming a cluster.

## Turn off transparent hugepages

To disable transparent hugepages, edit your grub.conf file (usually `/boot/grub/grub.conf`). Add the string `transparent\_hugepage=never` to the end of the `kernel` parameter for the instance you are using (see the instance that starts with `default=0` or the first one) in the following example:

```bash
# grub.conf generated by anaconda
#
# Note that you do not have to rerun grub after making changes to this file
# NOTICE:  You have a /boot partition.  This means that
#          all kernel and initrd paths are relative to /boot/, eg.
#          root (hd0,0)
#          kernel /vmlinuz-version ro root=/dev/mapper/VolGroup-lv_root
#          initrd /initrd-[generic-]version.img
#boot=/dev/sda
default=0
timeout=5
splashimage=(hd0,0)/grub/splash.xpm.gz
hiddenmenu
title CentOS (2.6.32-642.1.1.el6.x86_64)
	root (hd0,0)
	kernel /vmlinuz-2.6.32-642.1.1.el6.x86_64 ro root=/dev/mapper/VolGroup-lv_root rd_NO_LUKS LANG=en_US.UTF-8 rd_NO_MD rd_LVM_LV=VolGroup/lv_swap SYSFONT=latarcyrheb-sun16 crashkern
el=auto rd_LVM_LV=VolGroup/lv_root  KEYBOARDTYPE=pc KEYTABLE=us rd_NO_DM rhgb quiet transparent_hugepage=never
	initrd /initramfs-2.6.32-642.1.1.el6.x86_64.img
title CentOS (2.6.32-573.26.1.el6.x86_64)
	root (hd0,0)
	kernel /vmlinuz-2.6.32-573.26.1.el6.x86_64 ro root=/dev/mapper/VolGroup-lv_root rd_NO_LUKS LANG=en_US.UTF-8 rd_NO_MD rd_LVM_LV=VolGroup/lv_swap SYSFONT=latarcyrheb-sun16 crashker
nel=auto rd_LVM_LV=VolGroup/lv_root  KEYBOARDTYPE=pc KEYTABLE=us rd_NO_DM rhgb quiet
	initrd /initramfs-2.6.32-573.26.1.el6.x86_64.img
title CentOS 6 (2.6.32-504.el6.x86_64)
	root (hd0,0)
	kernel /vmlinuz-2.6.32-504.el6.x86_64 ro root=/dev/mapper/VolGroup-lv_root rd_NO_LUKS LANG=en_US.UTF-8 rd_NO_MD rd_LVM_LV=VolGroup/lv_swap SYSFONT=latarcyrheb-sun16 crashkernel=a
uto rd_LVM_LV=VolGroup/lv_root  KEYBOARDTYPE=pc KEYTABLE=us rd_NO_DM rhgb quiet
	initrd /initramfs-2.6.32-504.el6.x86_64.img
```
Transparent hugepages will not truly be disabled until the next reboot of the server. The instructions below do call for a reboot.

## Install the XFS tools

```bash
dbhost$ sudo yum -y install xfstools xfsdump
```

## Install or upgrade Java if necessary

Many sources agree that the tested and recent versions of Cassandra work best on Java 8, build 40 or higher. While we believe that versions of OpenJDK and Oracle 7 might work, Cassandra's requirement of Java 8 greater than Build 40 requires us to  run our tests with a recent release of Oracle Java 8.

Validate that Java is installed and at a reasonable version:


```bash
ycsbhost$ java -version
java version "1.8.0_92"
Java(TM) SE Runtime Environment (build 1.8.0_92-b14)
Java HotSpot(TM) 64-Bit Server VM (build 25.92-b14, mixed mode)
```

If you need to install or update Java to Oracle Version 8, there are many processes and help available on the Internet. The general process is to:
  1. Uninstall prior versions of Java through the `yum` system.
  2. Download the recent Java SE Development Kit RPM from [Oracle's download page]( http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html ).
  3. Install the RPM using the `rpm` command.

Other processes, such as compiling from source, downloading a binary build and decompressing into the `/opt` directory, or using non-Oracle repositories can also be used.

In the CentOS 6 installs we've seen, the following commands work, after having downloaded from Oracle's website:
```bash
ycsbhost$ sudo yum remove java-1.6.0-openjdk java-1.6.0-openjdk-headless java-1.6.0-openjdk-javadoc java-1.7.0-openjdk java-1.7.0-openjdk-headless java-1.7.0-openjdk-devel java-1.8.0-openjdk java-1.8.0-openjdk-headless java-1.8.0-openjdk-devel
ycsbhost$ sudo yum localinstall -y jdk-8u92-linux-x64.rpm
```

After you have updated, always be sure to validate using the `java -version` and `javac -version` that you are using the intended java version.

Make sure that your JAVA_HOME path is set by creating the appropriate file in `/etc/profile.d/java.sh` (your version may vary):

```bash
ycsbhost$ echo "export JAVA_HOME=/usr/java/jdk1.8.0_92" > /tmp/java.sh
ycsbhost$ sudo mv /tmp/java.sh /etc/profile.d
ycsbhost$ source /etc/profile.d/java.sh
```

{{/steps-step}}

{{!---
  ############################################################################
  #
  # STEP 3 - Install Python 2.7 and Virtualenv
  #
  ############################################################################
---}}
{{#steps-step 3 "Install Python 2.7" markdown=true}}

## Check your Python version

Cassandra requires Python 2.7 to run the CQLSH tools that will be used to test the
cluster and add tables for the test. You'll need to run scripts with this version after you form your cluster.

You'll need to run the tools on only one node in your cluster, or have a "master machine" that has an up-to-date distribution running the Cassandra tools.

As we are using CentOS 6, the procedure to install Python 2.7 is complex. If you are using a different distribution (such as CentOS 7, Ubuntu 14.04, or Debian 8), you'll already have Python 2.7. In this case, you may skip the instructions to install Python 2.7 and proceed directly to Step 4. 

Check your Python version as follows:

```bash
dbhost$ python
Python 2.6.6 (r266:84292, Jul 23 2015, 15:22:56)
[GCC 4.4.7 20120313 (Red Hat 4.4.7-11)] on linux2
Type "help", "copyright", "credits" or "license" for more information.
>>>
```

## Installing Python 2.7 on CentOS 6 - Internet Guides

The process for installing Python 2.7 is not trivial. You can't upgrade Python "in place" because CentOS' `yum` system relies on Python 2.6. A straight upgrade breaks `yum`, so you'll need to build (from scratch) a separate install.

Most of the Internet guides run to several pages. There are several that have worked for us in the past:

  * [Digital Ocean's guide](https://www.digitalocean.com/community/tutorials/how-to-set-up-python-2-7-6-and-3-3-3-on-centos-6-4) is reputable.

  * [Daniel Eriksson's guide](http://toomuchdata.com/2014/02/16/how-to-install-python-on-centos/) is also excellent, although our procedure differed slightly from his.

We chose the installation method below. If you follow one of the guides above, proceed directly to Step 4.

## Install Python 2.7 and Virtualenv to Run the Cassandra Tools

These instructions include quite a bit of information that is not required for the simple use we have in mind.

In order to just run the Cassandra tools, we suggest installing Python 2.7 to `/usr/local` (where it stays out of the way), and `virtualenv`, which we will use to create a runtime environment 
for running the CSQLSH tool.

## Install Python
Run the following command:

```bash
dbhost$ sudo yum -y install wget

# Install development tools some likely packages
dbhost$ sudo yum -y groupinstall "Development tools"
dbhost$ sudo yum -y install zlib-devel  # gen'l reqs
dbhost$ sudo yum -y install bzip2-devel openssl-devel ncurses-devel  # gen'l reqs
dbhost$ sudo yum -y install libxml2-devel libxslt-devel  # req'd by python package 'lxml'

# Fetch and install Python 2.7.9 to /usr/local
dbhost$ wget --no-check-certificate http://www.python.org/ftp/python/2.7.9/Python-2.7.9.tgz
dbhost$ tar xzf Python-2.7.9.tgz
dbhost$ cd Python-2.7.9
dbhost$ ./configure --prefix=/usr/local
dbhost$ sudo make && sudo make altinstall
```

## Install `virtualenv`

First, check to see whether you have `virtualenv` already installed:

```bash
dbhost$ whereis virtualenv
virtualenv:
```

This direct installation method has the fewest prerequisites. You can also use `pip install virtualenv` if you are using `pip`:

```bash
dbhost$ wget --no-check-certificate https://pypi.python.org/packages/source/v/virtualenv/virtualenv-15.0.1.tar.gz
dbhost$ tar xzf virtualenv-15.0.1.tar.gz
dbhost$ cd virtualenv-15.0.1
dbhost$ sudo python setup.py install
dbhost$ cd ..
```

Test `virtualenv`:

```bash
# check your python - should be old
dbhost$ python -V
Python 2.6.6
dbhost$ cd ~
# create a project folder - any name will do
dbhost$ mkdir my-proj
dbhost$ cd my-proj
# create an environment - any name will do
dbhost$ virtualenv venv
# direct it to python 2.7
dbhost$ virtualenv -p /usr/local/bin/python2.7 venv
# activate the virtual environment
dbhost$ source ~/my-proj/venv/bin/activate
# Check your python
dbhost$ python -V
Python 2.7.9
```

We'll use this later when we need the Cassandra CQL tool.

{{/steps-step}}

{{!---
  ############################################################################
  #
  # STEP 4 - Installing Cassandra
  #
  ############################################################################
---}}
{{#steps-step 4 "Installing Cassandra" markdown=true}}

Install Cassandra 3.5 to an `/opt/cassandra` directory:

1. Download from [Apache](http://cassandra.apache.org/download/ ),
  or use the following command:

  ```bash
  dbhost$ curl -O http://apache.cs.utah.edu/cassandra/3.5/apache-cassandra-3.5-bin.tar.gz
  ```

1. Create the `/opt/cassandra` directory, and decompress the package to it:

  ```bash
  dbhost$ sudo mkdir /opt/cassandra
  dbhost$ sudo tar -xf apache-cassandra-3.5-bin.tar.gz -C /opt/cassandra
  ```
{{/steps-step}}

{{!---
  ############################################################################
  #
  # STEP 5 - Preparing the data drives
  #
  ############################################################################
---}}
{{#steps-step 5 "Preparing the data drives" markdown=true}}

Cassandra requires both data directories and a commit log.

In our tests, we had 4 SSD devices. You may use more or fewer, though Cassandra's desire for a dedicated device as a commit log mandates two non-system devices. In our system, our devices were:
* /dev/sdb
* /dev/sdc
* /dev/sdd
* /dev/sde

If you are using Amazon, these will look like:
* /dev/xvdb
* /dev/xvdc

## Unmount any SSDs
In some cases, you may have SSDs that have already been mounted. Unmount them before proceeding. 

In the example below, using the command `lsblk`, we can see that xvdb is mounted at `/media/ephemeral0`.

```bash
dbhost$ lsblk
NAME    MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
xvda    202:0    0    18G  0 disk 
└─xvda1 202:1    0    18G  0 part /
xvdb    202:16   0   320G  0 disk /media/ephemeral0
```
To unmount the drive:

```bash
dbhost$ sudo umount /media/ephemeral0
```

## Zero the SSDs

Make sure your devices have been initialized. As flash devices retain state, the best tests will start with the drives at the same initial conditions:

```bash
dbhost$ sudo dd if=/dev/zero of=[DEVICE_ID] bs=1M
```

If you have multiple SSDs, you can zero them in parallel with the following command:

```bash
dbhost$ sudo dd if=/dev/zero of=/dev/stdout bs=1M | tee [FIRST_DEVICE] | tee [SECOND_DEVICE] > [FINAL_DEVICE]
```

## Set parameters for the SSDs

For each data SSD that you use, run the following:

```bash
dbhost$ sudo sh -c "echo 1 > /sys/block/[DEVICE_ID]/queue/nomerges"
dbhost$ sudo sh -c "echo 8 > /sys/block/[DEVICE_ID]/queue/read_ahead_kb"
dbhost$ sudo sh -c "echo deadline > /sys/block/[DEVICE_ID]/queue/scheduler"
```

## Partition the drives

In order to use XFS, you must correctly align the size of the partition. Using `fdisk`, create a single partition on each device. Make sure to enter the "units s" sub-command, as in the snippet below, although this may be obsolete in your version of CentOS/RedHat. You must start the first sector at "2048" and may end at the default value for the last sector.

Run `fdisk` by executing the following command:

```bash
dbhost$ sudo fdisk [DEVICE_ID]

The device presents a logical sector size that is smaller than
the physical sector size. Aligning to a physical sector (or optimal
I/O) size boundary is recommended, or performance may be impacted.

WARNING: DOS-compatible mode is deprecated. It's strongly recommended to
         switch off the mode (command 'c') and change display units to
         sectors (command 'u').

Command (m for help): units s
Changing display/entry units to sectors

Command (m for help): n
Command action
   e   extended
   p   primary partition (1-4)
p
Partition number (1-4):
Value out of range.
Partition number (1-4): 1
First sector (63-1562822319, default 64): 2048
Last sector, +sectors or +size{K,M,G} (2048-1562822319, default 1562822319):
Using default value 1562822319

Command (m for help): w
The partition table has been altered!

Calling ioctl() to re-read partition table.
Syncing disks.
```

Follow this pattern for each of your devices. You will now have partitions for each of your drives.

## Format each partition for XFS

We concluded that XFS is the current recommended file system for Cassandra.
You can use other file systems - such as BtrFS or Ext4 - which have different characteristics. Feel free to weigh in with your own results for different file systems. Now run the following command:

```bash
dbhost$ sudo mkfs.xfs -f /dev/sdb1
dbhost$ sudo mkfs.xfs -f /dev/sdc1
dbhost$ sudo mkfs.xfs -f /dev/sde1
dbhost$ sudo mkfs.xfs -f /dev/sde1
```
or, if using Amazon AWS:

```bash
dbhost$ sudo mkfs.xfs -f /dev/xvdb1
dbhost$ sudo mkfs.xfs -f /dev/xvdc1
```



## Get the UUIDs of the file systems
Now get the UUID of the SSD partitions you will be using (`/dev/sdb1` and `/dev/sdc1` below):
```bash
dbhost$ sudo blkid
/dev/mapper/vg_bbulkow-lv_root: UUID="2fb4278b-d3fc-45d2-b708-848722797b03" TYPE="ext4" 
/dev/sdb1: UUID="dbe66705-7d76-4283-aa73-c15a0b024687" TYPE="xfs"
/dev/sdc1: UUID="45e3d20a-433d-4541-a255-d85a2e7a160a" TYPE="xfs"
/dev/sda1: UUID="8921e1c0-8ccb-4102-951c-1403456ae2aa" TYPE="ext4"
/dev/sda2: UUID="JmQjGi-ZdgR-hAQv-5pvP-0zTb-MguA-e1Ifi8" TYPE="LVM2_member"
```

## Create directories for each mount point
You may create arbitrary directory names, but we suggest the following:
```bash
dbhost$ sudo mkdir -p /var/lib/cassandra/data/sdb1
dbhost$ sudo mkdir -p /var/lib/cassandra/data/sdc1
```

## Add these to your fstab file to mount them permanently
Enter the following command:

```bash
sudo vi /etc/fstab
```

```asciidoc
UUID=dbe66705-7d76-4283-aa73-c15a0b024687 /var/lib/cassandra/data/sdb1 xfs defaults 0 2
UUID=45e3d20a-433d-4541-a255-d85a2e7a160a /var/lib/cassandra/data/sdc1 xfs defaults 0 2
```

## Reboot and validate that your data directories are correct

Reboot your server and make sure that your SSDs are properly mounted. 

{{/steps-step}}


{{!---
  ############################################################################
  #
  # STEP 6 - Tuning your Kernel
  #
  ############################################################################
---}}
{{#steps-step 6 "Tuning your Kernel" markdown=true}}

In order to run Cassandra for a 12-hour load test, we found that we had to set a number of system variables.

## SysCtl variables

Please add the following lines to the bottom of the `/etc/sysctl.conf` file:

```
# Controls the maximum number of shared memory segments, in pages
kernel.shmall = 4294967296

net.ipv4.ip_local_port_range = 10000 65535
net.ipv4.tcp_window_scaling = 1
net.ipv4.tcp_rmem = 4096 87380 16777216
net.ipv4.tcp_wmem = 4096 65536 16777216
net.core.rmem_max = 16777216
net.core.wmem_max = 16777216
net.core.netdev_max_backlog = 2500
net.core.somaxconn = 65000

# significantly reduce the amount of data the kernel is allowed to store
# in memory between fsyncs
# dirty_background_bytes says how many bytes can be dirty before the kernel
# starts flushing in the background. Set this as low as you can get away with.
# It is basically equivalent to trickle_fsync=true but more efficient since the
# kernel is doing it. Older kernels will need to use vm.dirty_background_ratio
# instead.
vm.dirty_background_bytes = 16777216

# Same deal as dirty_background but the whole system will pause when this threshold
# is hit, so be generous and set this to a much higher value, e.g. 1GB.
# Older kernels will need to use dirty_ratio instead.
vm.dirty_bytes = 4294967296

# disable zone reclaim for IO-heavy applications like Cassandra
vm.zone_reclaim_mode = 0

# there is no good reason to limit these on server systems, so set them
# to 2^31 to avoid any issues
# Very large values in max_map_count may cause instability in some kernels.
fs.file-max = 1073741824
vm.max_map_count = 1073741824

# only swap if absolutely necessary
# some kernels have trouble with 0 as a value, so stick with 1
vm.swappiness = 1
```

## Increase the number of file descriptors

Add these lines to `/etc/security/limits.conf`:

```asciidoc
root soft nofile 100000
root hard nofile 100000
cassandra soft nofile 100000
cassandra hard nofile 100000
```

Then, set the new settings by running:

```bash
dbhost$ sudo sysctl -p
```
The OS configuration is now complete.

{{/steps-step}}


{{!---
  ############################################################################
  #
  # STEP 7 - Basic Cassandra Configuration
  #
  ############################################################################
---}}
{{#steps-step 7 "Basic Cassandra Configuration" markdown=true}}

It is easier to configure the system if you change ownership of the Cassandra directory to your current user. You can change "centos" to your user (this is not necessary if running as root):

```bash
dbhost$ sudo chown -R centos:centos /opt/cassandra
```

##Edit your cassandra.yaml file
Use your favorite editor on the main Cassandra configuration file, `/opt/cassandra/apache-cassandra-3.5/conf/cassandra.yaml`.

### Name your cluster
Look for `cluster_name` in the file and set it as follows:

```bash
cluster_name: 'YCSB Cluster'
```

### Add the directories that will contain the data files
You must also make sure to uncomment the "data_file_directories" line:

```bash
data_file_directories:
    - /var/lib/cassandra/data/sdb1
    - /var/lib/cassandra/data/sdc1
```

### Add your commitlog
Leave this commented out if you do not have a device for the commitlog. Otherwise, use one of the data drives as the commitlog:

```bash
commitlog_directory: /var/lib/cassandra/data/sdb1/commitlog
```

### Add one or more seed nodes, but don't add this node
The seed IP will be the IP address of one of the other nodes in your cluster. If you are on Amazon AWS, make sure to use that node's private IP address:

```bash
seed_provider:
    - class_name: org.apache.cassandra.locator.SimpleSeedProvider
      parameters:
          # seeds is a comma-delimited list of addresses.
          # Ex: "<ip1>,<ip2>,<ip3>"
          - seeds: "[SEED_IP]"
```

### Set the listen address to the local address or to the interface, but not both
You will need to tell the node which IP address (or network interface) to listen on. You may choose ONE of the following:

```bash
listen_address: [LOCAL_IP]
```
or

```bash
listen_interface: [NETWORK_DEVICE]
```
where NETWORK_DEVICE is the device ID of your NIC (e.g. "eth0").


### Set the RPC Address and the RPC Broadcast Address
Set the `rpc_address` parameter to "0.0.0.0":

```bash
rpc_address: 0.0.0.0
```

and uncomment the `broadcast_rpc_address` line:

```bash
broadcast_rpc_address: 1.2.3.4
```

### Add `auto_bootstrap` at the end of the file

Finally, change the bootstrap setting in the last line of the file:

```bash
auto_bootstrap: false
```

{{/steps-step}}


{{!---
  ############################################################################
  #
  # STEP 8 - Tuning Cassandra for a 12-hour Benchmark
  #
  ############################################################################
---}}
{{#steps-step 8 "Tuning Cassandra for a 12-hour Benchmark" markdown=true}}

## Increase the heap size

In `/opt/cassandra/apache-cassandra-3.5/conf/cassandra-env.sh`, uncomment the
following fields and set reasonable values. Note that there are 2 different locations for "MAX_HEAP_SIZE", so look for the second instance, immediately followed by "HEAP_NEWSIZE":

``` 
MAX_HEAP_SIZE="8G"
HEAP_NEWSIZE="1600M"
```

## Increase the number of threads and optimize for SSD

Edit your `/opt/cassandra/apache-cassandra-3.5/conf/cassandra.yaml` file.

### Increase the concurrent reads and writes

Suggested settings are for 16x the number of SSDs for reads and 8x the number of cores for concurrent writes. In our benchmark, we used 4 SSDs and had 16 (physical) cores, so our settings were for 64 concurrent reads and 128 concurrent writes, as follows:

```bash
concurrent_reads: 64
concurrent_writes: 128
```

### Set the disk optimization strategy to SSD

```bash
disk_optimization_strategy: ssd
```

### As recommended for SSD, increase the number of flush writers
Suggested settings are for this to be equal to the number of (physical) cores. 
```bash
memtable_flush_writers: 16
```

### Set the trickle fsync value to true

```bash
trickle_fsync: true
```

### Increase the number of compactors
Set the number of concurrent compactors to be equal to the number of (physical) cores. We chose to use continual compaction (`compaction_throughput_mb_per_sec=0`) for this stress test:
```bash
concurrent_compactors: 16
compaction_throughput_mb_per_sec: 0
```

### Change the JVM options

Now edit the file `/opt/cassandra/apache-cassandra-3.5/conf/jvm.options`. We want to do the following, as shown in the example below:

  * Disable the CMS systems
  * Enable the HotSpot G1 garbage-first collector
  * Enable the optional G1 settings



``` 
#### CMS Settings

#-XX:+UseParNewGC
#-XX:+UseConcMarkSweepGC
#-XX:+CMSParallelRemarkEnabled
#-XX:SurvivorRatio=8
#-XX:MaxTenuringThreshold=1
#-XX:CMSInitiatingOccupancyFraction=75
#-XX:+UseCMSInitiatingOccupancyOnly
#-XX:CMSWaitDuration=10000
#-XX:+CMSParallelInitialMarkEnabled
#-XX:+CMSEdenChunksRecordAlways
# some JVMs will fill up their heap when accessed via JMX, see CASSANDRA-6541
#-XX:+CMSClassUnloadingEnabled

### G1 Settings (experimental, comment previous section and uncomment section below to enable)

## Use the Hotspot garbage-first collector.
-XX:+UseG1GC
#
## Have the JVM do less remembered set work during STW, instead
## preferring concurrent GC. Reduces p99.9 latency.
-XX:G1RSetUpdatingPauseTimePercent=5
#
## Main G1GC tunable: lowering the pause target will lower throughput and vise versa.
## 200ms is the JVM default and lowest viable setting
## 1000ms increases throughput. Keep it smaller than the timeouts in cassandra.yaml.
-XX:MaxGCPauseMillis=500

## Optional G1 Settings

# Save CPU time on large (>= 16GB) heaps by delaying region scanning
# until the heap is 70% full. The default in Hotspot 8u40 is 40%.
-XX:InitiatingHeapOccupancyPercent=25

# For systems with > 8 cores, the default ParallelGCThreads is 5/8 the number of logical cores.
# Otherwise equal to the number of cores when 8 or less.
# Machines with > 10 cores should try setting these to <= full cores.
-XX:ParallelGCThreads=16
# By default, ConcGCThreads is 1/4 of ParallelGCThreads.
# Setting both to the same value can reduce STW durations.
-XX:ConcGCThreads=16

### GC logging options -- uncomment to enable

-XX:+PrintGCDetails
-XX:+PrintGCDateStamps
-XX:+PrintHeapAtGC
-XX:+PrintTenuringDistribution
-XX:+PrintGCApplicationStoppedTime
-XX:+PrintPromotionFailure
#-XX:PrintFLSStatistics=1
#-Xloggc:/var/log/cassandra/gc.log
-XX:+UseGCLogFileRotation
-XX:NumberOfGCLogFiles=10
-XX:GCLogFileSize=10M
```

{{/steps-step}}


{{!---
  ############################################################################
  #
  # STEP 9 - Start/Stop Cassandra
  #
  ############################################################################
---}}
{{#steps-step 9 "Start/Stop Cassandra" markdown=true}}

## Start the servers

On each node in the cluster, run:

```bash
dbhost$ cd /opt/cassandra/apache-cassandra-3.5/bin
dbhost$ sudo nohup ./cassandra -R &
```

## Validate that the server has started

```bash
dbhost$ ./nodetool status
```

{{/steps-step}}

{{!---
  ############################################################################
  #
  # STEP 10 - Add KEYSPACE and TABLE for YCSB
  #
  ############################################################################
---}}
{{#steps-step 10 "Add KEYSPACE and TABLE for YCSB" markdown=true}}

For this step, you'll need to have successfully completed Step 3, "Installing Python 2.7". If you still need to install
Python 2.7, activate the virtual environment that you created earlier (this step is not necessary if you are using the Aerospike Amazon AMI):

```bash
dbhost$ source ~/my-proj/venv/bin/activate
dbhost$ python -V
Python 2.7.9
```

Launch the CQLSH tool:

```bash
dbhost$ cd /opt/cassandra/apache-cassandra-3.5/bin
dbhost$ ./cqlsh.py
```

Create a keyspace and table for use with the YCSB workloads we intend to utilize:

```sql
cqlsh> create keyspace ycsb WITH REPLICATION = {'class' : 'SimpleStrategy', 'replication_factor': 2 };
cqlsh> create table ycsb.usertable (y_id varchar primary key, field0 blob, field1 blob, field2 blob, field3 blob, field4 blob, field5 blob, field6 blob, field7 blob, field8 blob, field9 blob) with compression = {'sstable_compression': ''}; 
cqlsh> ALTER TABLE ycsb.usertable WITH compaction = { 'class' :  'LeveledCompactionStrategy'  };
```

{{/steps-step}}
{{!---
  ############################################################################
  #
  # STEP 11 - Next Steps
  #
  ############################################################################
---}}
{{#steps-step 11 "Next Steps" markdown=true}}

Now you can run the benchmark. Go to the [Running YCSB on Cassandra Page](/docs/benchmarks/cassandra/simple_ycsb/ycsb_cassandra.html) for instructions on running the actual benchmark.

{{/steps-step}}

{{/steps}}
